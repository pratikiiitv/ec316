CS316 Optimisation Methods
Monday & Wednesday 9:15 - 10:45 AM

Evaluation Policy
Mid Examination : 30% (Pen paper)
End Examination : 45% (Pen paper)
Term Paper : 25%

Books
Bertsimas, Dimitris, and John Tsitsiklis. Introduction to Linear Optimization. Belmont, MA: Athena Scientific, 1997. ISBN: 9781886529199.
David G. Luenberger and Yinyu Ye. Linear and Nonlinear Programming. Springer, 2008, Third Edition.

---
Lecture 1 : 05/01/2025
Example Linear Program
Normal Form Linear Program

Production Problem : Computer Manufacturer
Multiperiod Planning of Electric Power Capacity
Scheduling Problem
Choosing Paths in a Communication Network
Pattern Classification

Piecewise Linear Objective Function
Problems involving Absolute Values
Data Fitting
Optimal Control of Linear Systems
Rocket Control

Graphical Representation and Solution
Visualizing Standard Form Problems

Exercises (Chapter 1 of textbook)

---

Lecture 2 : 07/01/2025

Linear Program in Standard Form
Definitions:
Polyhedron, Halfspace, Hyperplane
Function and its epigraph
Convexity - Continuity?
Abstraction to metric space - convexity (vector space)
Convex hull, Convex combination
Extreme Point

---

Lecture 3 : 13/01/2025

Functions
f : \mathbb{R} \to \mathbb{R}, y = f(x)
Ex f(x) = ax

Continuity at a point
f is continuous at x_0 if \forall \epsilon>0 \exists \delta>0
such that, 
|f(x) - f(x_0)|< \epsilon \implies |x - x_0| < \delta.
-Extension-
d(f(x), f(x_0))<\epsilon \implies d(x, x_0)< \delta


Equality is defined in terms of inequality
x = y 
1 + 1/2 + 1/4 + 1/8 + ... = 2
|x-y|<\epsilon , \forall \epsilon>0

f:\mathbb{R}\to\mathbb{R}
f(x) = x^2
Minimum is in domain
Minimum value is in range
Local minimum vs global minimum

f(x) = (x-1)^2 
f(x) = (x-1)^2 + 5
f(x) = (x-1)^2 + (x+1)^2
f(x) = 2*x^2
f(x) = -x^2



f(x) = ax^2 a>0/ a<0
Derivative df/dx = 4x = 0 ??
Double derivative d^2f/dx^2 = -2

f(x) = x^Tx, x\in \mathbb{R^2}
    = [x_1 x_2][x_1 x_2]^T
    = x_1^2 + x_2^2
f(x) = x^TAx, x\in \mathbb{R^2}, A\in \mathbb{R^{2x2}}
A = [1 0; 0 3];
A = [-1 0; 0 -3];
A = [1 0; 0 -3];
---
Matlab (for plotting):
clear all;
close all;
x = [-10:.5:10];
A = [1 0; 0 -1];
for i = 1:length(x)
    for j = 1:length(x)
        f(i,j) = [x(i) x(j)]*A*[x(i) x(j)]';
    end
end
---

Lecture 4 : 15/01/2025

What if A has only one non-zero eigenvalue?
A = [1 0; 0 0]
f(x_1,x_2) = x_1^2
Another way to look at functions of two variables: 
Contour plot/ Level Set

f:\mathhb{R^n} \to \mathbb{R}
Lf(v) = {x | f(x)=v, x\in \mathbb{R^2}}

Important Concepts : Optimization
(A) Convexity
- Newton's Method (First & Second Derivatives)
- Gradient Method (First Derivative only)
 
(B) Lagrange Multipliers
- Theory of Duality

(C) Linear Program, Quadratic Program, SDP, Cone Programming

Argmin/ Min
argmin f(x) = value(s) of x where f reaches minimum

One variable: 

f(x), f'(x)=df/dx, ..., f^n(x) = (d/dx)^nf
f(x+dx) = f(x) + dx f'(x) + (1/2) (dx)^2 f''(x) + ... 
f(x+dx) ~ f(x) + dx f'(x) + (1/2) (dx)^2 f''(x) 

Two variables:
f : \mathbb{R^n} \to \mathbb{R}
f(x) := f(x_1,x_2)
f'(x) = Grad f(x) = [f_1(x) f_2(x)]' := Gf
f''(x) = [f_11(x) f_21(x); f_12(x) f_22(x)]' (Hessian of f) := Hf
f_21(x) = d/dx_2 (f_1(x)), where f_1(x) = d/dx_1 (f(x))

f(x+dx) ~ f(x) + dx f'(x) + (1/2) (dx)^2 f''(x) 
f(x+dx) ~ f(x) + dx^T Gf(x) + (1/2) dx^T Hf(x) dx

Ax = \lamda x
d/dx (e^ax) = a*(e^ax)

sin(2*pi*f*t) = (1/2i) * [e^(i*2*pi*f*t) - e^(-i*2*pi*f*t)]
d/dt (sin(2*pi*f*t)) = (pi*f) [e^(i*2*pi*f*t) + e^(-i*2*pi*f*t)]
 = (2*pi*f*e^(-i*pi/2)) sin(2*pi*f*t)

* Code to rectify!
clear all;
close all;

x = [-10:.5:10];
A = [1 0; 0 1];


for i = 1:length(x)
    for j = 1:length(x)
        f(i,j) = [x(i) x(j)]*A*[x(i) x(j)]';
        df1(i,j) = 2*x(i);    
        df2(i,j) = 2*x(j);
     end
end

[x1,x2] = meshgrid(x,x);
figure(1);
contour(x1,x2,f); hold on;
quiver(x1, x2, df1, df2);



Jacobian: J(x)
f : \mathbb{R^n} \to \mathbb{R^m}
m functions (f_1, f_2,..., f_m)
n variables (x_1, x_2, ..., x_n)

f(x+dx) ~ f(x) + J_f(x) dx
J_f(x) = [f_11 f_12 ... f_1n; f_21, ..., f_2n; ...; f_m1, ..., f_mn ]

!! Hessian of a real function of n variables is Jacobian of the Gradient
of the function.

---

Lecture 5 : 20/01/2025

Four fundamental subspaces of a Matrix
A : \mathbb{R^m} \to \mathbb{R^n}
A.shape = n x m

Span of a set of vectors
Rank of a Matrix - r
- Number of independent columns
- Number of independent rows
- Number of nonzero eigenvalues
- Dimension of column space

Column space of A := {y | y = Ax, x\in \mathbb{R^n}}, Span{Col(A)}
Left Null Space of A := {y | A^T y = 0, y\in \mathbb{R^m}}
Null space of A := {x | Ax = 0, x \in \mathbb{R^n}}
Row space of A := Span{Rows(A)}

n = dim(Null(A)) + r 
m = dim(Null(A^T)) + r

Ax_1 = y_1
Ax_0 = 0
A(x_1+x_0) = Ax_1 + Ax_0 = y_1

Gaussian Elimination
Trace of a Matrix
Singular Value Decomposition

---
Quadratic Unconstrained Optimization
f : \mathbb{R^2} \to \mathbb{R}
f(x) = x^T A x + b^T x + C
A \in \mathbb{R^{2x2}}, b\in \mathbb{R^2}, c\in \mathbb{R}

GD : First Attempt
f, df, x_0
x_{k+1} = x_k - step_size * df(x_k)
|| x_{k+1} - x_k || < espilon

Issues with stepsize and epsilon
- convergence?
- Gradient normalization?
- Cauchy sequences and sequence convergence

Neural Network
Training Set := {x_i, y_i}, x_i\in \mathbb{R^m}, y_i\in \mathbb{R^n}
f(x_i, \theta ) = y_i, \forall i \in \indx{Training Set} 

minimize \Sum_i ||f(x_i, \theta) - y_i||
f(x_i, a, b) = a*x_i + b
||f(x_i, \theta) - y_i||
\Sum_i (a*x_i + b - y_i)^2 
\phi(a,b) = x_i^2 * a^2 + b^2 + y_i^2 + 2*a*b*x_i - 2*a*x_i*y_i - 2*b*y_i

Q: Given three points in \mathbb{R^2}, {[1,  1], [2, 3], [4, 4]} 
1. Draw a straight line passing through these three points. 
2. Think of the points as [x_i, y_i] pairs. And straight line as a function 
relationship betwen x_i and y_i, such that f(x_i) = y_i
3. Write down the quadratic cost function.
4. Try solving the same using First Attempt GD.















