CS316 Optimisation Methods
Monday & Wednesday 9:15 - 10:45 AM

Evaluation Policy
Mid Examination : 30% (Pen paper)
End Examination : 45% (Pen paper)
Term Paper : 25%

Books
Bertsimas, Dimitris, and John Tsitsiklis. Introduction to Linear Optimization. Belmont, MA: Athena Scientific, 1997. ISBN: 9781886529199.
David G. Luenberger and Yinyu Ye. Linear and Nonlinear Programming. Springer, 2008, Third Edition.

---
Lecture 1 : 05/01/2025
Example Linear Program
Normal Form Linear Program

Production Problem : Computer Manufacturer
Multiperiod Planning of Electric Power Capacity
Scheduling Problem
Choosing Paths in a Communication Network
Pattern Classification

Piecewise Linear Objective Function
Problems involving Absolute Values
Data Fitting
Optimal Control of Linear Systems
Rocket Control

Graphical Representation and Solution
Visualizing Standard Form Problems

Exercises (Chapter 1 of textbook)

---

Lecture 2 : 07/01/2025

Linear Program in Standard Form
Definitions:
Polyhedron, Halfspace, Hyperplane
Function and its epigraph
Convexity - Continuity?
Abstraction to metric space - convexity (vector space)
Convex hull, Convex combination
Extreme Point

---

Lecture 3 : 13/01/2025

Functions
f : \mathbb{R} \to \mathbb{R}, y = f(x)
Ex f(x) = ax

Continuity at a point
f is continuous at x_0 if \forall \epsilon>0 \exists \delta>0
such that, 
|f(x) - f(x_0)|< \epsilon \implies |x - x_0| < \delta.
-Extension-
d(f(x), f(x_0))<\epsilon \implies d(x, x_0)< \delta


Equality is defined in terms of inequality
x = y 
1 + 1/2 + 1/4 + 1/8 + ... = 2
|x-y|<\epsilon , \forall \epsilon>0

f:\mathbb{R}\to\mathbb{R}
f(x) = x^2
Minimum is in domain
Minimum value is in range
Local minimum vs global minimum

f(x) = (x-1)^2 
f(x) = (x-1)^2 + 5
f(x) = (x-1)^2 + (x+1)^2
f(x) = 2*x^2
f(x) = -x^2



f(x) = ax^2 a>0/ a<0
Derivative df/dx = 4x = 0 ??
Double derivative d^2f/dx^2 = -2

f(x) = x^Tx, x\in \mathbb{R^2}
    = [x_1 x_2][x_1 x_2]^T
    = x_1^2 + x_2^2
f(x) = x^TAx, x\in \mathbb{R^2}, A\in \mathbb{R^{2x2}}
A = [1 0; 0 3];
A = [-1 0; 0 -3];
A = [1 0; 0 -3];
---
Matlab (for plotting):
clear all;
close all;
x = [-10:.5:10];
A = [1 0; 0 -1];
for i = 1:length(x)
    for j = 1:length(x)
        f(i,j) = [x(i) x(j)]*A*[x(i) x(j)]';
    end
end
---

Lecture 4 : 15/01/2025

What if A has only one non-zero eigenvalue?
A = [1 0; 0 0]
f(x_1,x_2) = x_1^2
Another way to look at functions of two variables: 
Contour plot/ Level Set

f:\mathhb{R^n} \to \mathbb{R}
Lf(v) = {x | f(x)=v, x\in \mathbb{R^2}}

Important Concepts : Optimization
(A) Convexity
- Newton's Method (First & Second Derivatives)
- Gradient Method (First Derivative only)
 
(B) Lagrange Multipliers
- Theory of Duality

(C) Linear Program, Quadratic Program, SDP, Cone Programming

Argmin/ Min
argmin f(x) = value(s) of x where f reaches minimum

One variable: 

f(x), f'(x)=df/dx, ..., f^n(x) = (d/dx)^nf
f(x+dx) = f(x) + dx f'(x) + (1/2) (dx)^2 f''(x) + ... 
f(x+dx) ~ f(x) + dx f'(x) + (1/2) (dx)^2 f''(x) 

Two variables:
f : \mathbb{R^n} \to \mathbb{R}
f(x) := f(x_1,x_2)
f'(x) = Grad f(x) = [f_1(x) f_2(x)]' := Gf
f''(x) = [f_11(x) f_21(x); f_12(x) f_22(x)]' (Hessian of f) := Hf
f_21(x) = d/dx_2 (f_1(x)), where f_1(x) = d/dx_1 (f(x))

f(x+dx) ~ f(x) + dx f'(x) + (1/2) (dx)^2 f''(x) 
f(x+dx) ~ f(x) + dx^T Gf(x) + (1/2) dx^T Hf(x) dx

Ax = \lamda x
d/dx (e^ax) = a*(e^ax)

sin(2*pi*f*t) = (1/2i) * [e^(i*2*pi*f*t) - e^(-i*2*pi*f*t)]
d/dt (sin(2*pi*f*t)) = (pi*f) [e^(i*2*pi*f*t) + e^(-i*2*pi*f*t)]
 = (2*pi*f*e^(-i*pi/2)) sin(2*pi*f*t)

* Code to rectify!
clear all;
close all;

x = [-10:.5:10];
A = [1 0; 0 1];


for i = 1:length(x)
    for j = 1:length(x)
        f(i,j) = [x(i) x(j)]*A*[x(i) x(j)]';
        df1(i,j) = 2*x(i);    
        df2(i,j) = 2*x(j);
     end
end

[x1,x2] = meshgrid(x,x);
figure(1);
contour(x1,x2,f); hold on;
quiver(x1, x2, df1, df2);



Jacobian: J(x)
f : \mathbb{R^n} \to \mathbb{R^m}
m functions (f_1, f_2,..., f_m)
n variables (x_1, x_2, ..., x_n)

f(x+dx) ~ f(x) + J_f(x) dx
J_f(x) = [f_11 f_12 ... f_1n; f_21, ..., f_2n; ...; f_m1, ..., f_mn ]

!! Hessian of a real function of n variables is Jacobian of the Gradient
of the function.

---

Lecture 5 : 20/01/2025

Four fundamental subspaces of a Matrix
A : \mathbb{R^m} \to \mathbb{R^n}
A.shape = n x m

Span of a set of vectors
Rank of a Matrix - r
- Number of independent columns
- Number of independent rows
- Number of nonzero eigenvalues
- Dimension of column space

Column space of A := {y | y = Ax, x\in \mathbb{R^n}}, Span{Col(A)}
Left Null Space of A := {y | A^T y = 0, y\in \mathbb{R^m}}
Null space of A := {x | Ax = 0, x \in \mathbb{R^n}}
Row space of A := Span{Rows(A)}

n = dim(Null(A)) + r 
m = dim(Null(A^T)) + r

Ax_1 = y_1
Ax_0 = 0
A(x_1+x_0) = Ax_1 + Ax_0 = y_1

Gaussian Elimination
Trace of a Matrix
Singular Value Decomposition

---
Quadratic Unconstrained Optimization
f : \mathbb{R^2} \to \mathbb{R}
f(x) = x^T A x + b^T x + C
A \in \mathbb{R^{2x2}}, b\in \mathbb{R^2}, c\in \mathbb{R}

GD : First Attempt
f, df, x_0
x_{k+1} = x_k - step_size * df(x_k)
|| x_{k+1} - x_k || < espilon

Issues with stepsize and epsilon
- convergence?
- Gradient normalization?
- Cauchy sequences and sequence convergence

Neural Network
Training Set := {x_i, y_i}, x_i\in \mathbb{R^m}, y_i\in \mathbb{R^n}
f(x_i, \theta ) = y_i, \forall i \in \indx{Training Set} 

minimize \Sum_i ||f(x_i, \theta) - y_i||
f(x_i, a, b) = a*x_i + b
||f(x_i, \theta) - y_i||
\Sum_i (a*x_i + b - y_i)^2 
\phi(a,b) = x_i^2 * a^2 + b^2 + y_i^2 + 2*a*b*x_i - 2*a*x_i*y_i - 2*b*y_i

Q: Given three points in \mathbb{R^2}, {[1,  1], [2, 3], [4, 4]} 
1. Draw a straight line passing through these three points. 
2. Think of the points as [x_i, y_i] pairs. And straight line as a function 
relationship betwen x_i and y_i, such that f(x_i) = y_i
3. Write down the quadratic cost function.
4. Try solving the same using First Attempt GD.

---

Lecture 6 : 22/01/2025

Vector Space
- Linear Combinations
- Linear Independence of a set of vectors
- Functions from vector space to vector space
- Dimensionality of vector space
- Set of basis vector := basis
- Finite Dimensional Vector Space
- Representation theorem: Every linear function from FDV to FDV
- Linear functions as matrices
- Subspaces related to Matrix (Rol, Col, Null, Left Null)

Inner Product Space
- Orthogonality
- <x|y>, x,y \in V (Inner product)
- |x><y|, outer Product

Normed Vector Space
Shape of Vector Norms!
\mathcal{C}(\mathbb{R}) : {Set of all continuous real functions
on \mathbb{R}}

\mathcal((0,1))

- L_p (Lebesgue Integration)
- l_p  

V:=\mathbb{R^n}
l_p(v) = (sum_i (|v_i|)^p)^{1/p}

- circle -> sphere
- disk -> ball
- open disk -> open ball

* (0,1) [0,1) [0,1]

Ex: n=2, p=2
?{v | l_p(v) = 1} : = {v_1^2+v_2^2 = 1} (circle)
?{v | l_p(v) <= 1} is convex

Ex: n=2, p=1
?{v | l_p(v) <= 1} is convex
l_1(v) = sum_i abs(v_i)

Ex: n=2, p=3
?{v | l_p(v) <= 1} is convex

Ex: n=2, p=\infinity
l_\infinity(v) = sup(abs(v_i))
A = {0.1, .5, .7} 
A = {0, 1-1/2, 1-1/3, ..., 1-1/n, ... }
sup := least upper bound

Ex: n=2, p=0
?l_0(v) = 2
l_0(v) = |support(v)|
Sparsity norm * (misnormer)

Ex:
u = [1 2 3] 
v = [1 1 1]
p = {0, 1, 2, \inf }
||u-v||_p = {2, 3, sqrt(5), 2}

Ex:
S = {x | x_1+x_2 = 3}
Find a point in S closest to [0 0] 
w.r.t. p norms p:={0,1,2,\inf}

Limit points and boundary

---

Lecture 7 : 27/01/2025

A point is said to be inside the set (interior points) if
it has a neighborhood in the set.

Strict convexity (< inplace of <=)

Projection : P 
In the context of vector spaces - we have looked at 
projections on subspaces, i.e. hyperplanes passing through
origin

Ex. In \mathbb{R^2}, a^Tx = 0 a,x \in \mathbb{R^2}
a = [1 1]' - 1 D
a = [1 1 1]' - 2 D 
a = [1 1 1 1]' - 3 D 

Ex. Find a closest (l_2) point to origin on a^Tx+b=0, for a = [1 1 1]' 
in \mathbb{R^3} where b = 1

min ||x||^2
(A) s.t. a^Tx+b <= 0
(B) s.t. a^Tx+b >= 0
(C) s.t. a^Tx+b = 0

min x_1^2+x_2^2+x_3^2
s.t. x_1+x_2+x_3 + 1 = 0

<a|x> = a^Tx 

x=c*a such that this will be in a^Tx+b=0
a^T(c*a) + b = 0
c = -b/(a^Ta)
x = (-b/(a^Ta)) * a
x = (-b/||a||) * a_unit

Q Find a closest (l_2) point (wrt origin) on a^Tx+b=0, for a = [1 1 1]' 
in \mathbb{R^3} where b = 1
min ||x-[0 0 0]'||^2
s.t. a^Tx+b = 0
Ans: x = (-b/||a||) * a_unit

Q Find a closest (l_2) point (wrt x_0=[2 3 4]') on a^Tx+b=0, for a = [1 1 1]' 
in \mathbb{R^3} where b = 1
min ||x-x_0'||^2
s.t. a^Tx+b = 0
Ans is called projection of x_0 onto S:={x | a^Tx+b=0}
y = x-x_0'
min ||y||^2
s.t. a^T(y+x_0') + b =0
s.t. a^Ty + (b+a^Tx_0') = 0
s.t. a^Ty + b_ = 0
y = (-b_/a^Ta) * a
y = (-b-a^Tx_0 / a^Ta) * a
y = (-b/(a^Ta)) * a - (a_unit^Tx_0) * a_unit 

Quiz : Code
S_1 := {x | a^Tx + b = 0}
S_2 := {x | c^Tx + d = 0}
a, c \in \mathbb{R^2}, b, d \in \mathbb{R}

Start from x_0 and and sequentially project on S_1 and S_2 
till convergence
1. x_k := Projection of x_k-1 on S_1
2. x_k+1 := Projection of x_1=k on S_2 

A point, a convex set, a strict norm, and we get 
a unique projection!

---
Lecture 8 : 29/01/2025

{x_k}, k\in{0,1,...}
Rate of convergence: 
||x_k-x_{k-1}||/||x_{k-1}-x_{k-2}||

<x|y> = conjugate(x)^T * y, if x,y\in \mathbb{C}^n 
Visual Complex Analysis, Tristan Needham

*Neural Network - Learning Weights

min f(x)
s.t. <a|x>=b : S_1

f(x) = x^TAx, x\in\mathbb{R}^2
A = [1 1, 1 3];
a = [-1 3]^T;
b = 4;

Projected Gradient Descent
Initialize - x_0, lamda = 0.1
Step 1: x_1 = Project_S_1(x_0)
Step 2: x_2 = x_1 - 0.1*grad(f(x_1))
Repeat 

Matrix Derivatives
f(x), f'(x) 
f(x) = ax^2, f'(x) = 2*ax
f(x) = x^TAx, f'(x) = 2*A^Tx, if A is symmetric

f(x) = [x1 x2] * A * [x1 x2]^T 
= [x1a11+x2a21 x1a12+x2a22] * [x1 x2]^T 
= x1a11x1+x2a21x1+x1a12x2+x2a22x2

df/dx1 = 2a11x1 + (a21 + a12)x2 
df/dx2 = a21x1 + a12x1 + 2a22x2

df/dx = [2a11 a12+a21; a21+a12 2a22] * [x1 x2]^T
if A is symmetric a12 = a21
df/dx = 2[a11 a21; a12 a22]*x

(Matrix Cookbook!)

Single Neuron
x -> f_{w,b}() -> sigma(<x|w>+b)
{x_i, y_i} i\in I Training set
{{[1 1],  1}, {[2 3], 1}, {[-1 0], -1}}
sigma(x) = x 

min ||y_1 - f_{w,b}(x_1)|| + ...
s.t. ||w||=1

\phi(x,y) = ||y-f(x)||
min \phi(x_1,y_1) + \phi(x_2,y_2) + ...

min \phi_1(w,b) + \phi_2(w,b) + \phi_3(w,b) + ...
s.t ||w||=1

Initialization : x_0, 
Till convergence
for i=1:3
Step 1: x_1 = x_0 - lamda * grad(\phi_i)
Step 2: x_2 = Project_S_1(x_1)
Repeat

---

Lecture 9 : 10/02/2025

Ex. 
min 1/2 x^2
s.t 2x >= 1

(Dual Variable - \alpha)

L(x,\alpha) = 1/2 x^2 + \alpha (-2x+1), \alpha>=0
? max L(x,\alpha)
s.t. \alpha >= 0
x \in feasible, what is \alpha? \alpha = 0
max L(x,0) = 1/2 x^2 -> objective function!
x \in infeasible, what is \alpha? \alpha = \infinity
max L(x,\infinity) = \infinity

min     max     L(x, \alpha)
x       \alpha>=0 

max     min     L(x, \alpha)
\alpha>=0 x


L(x,y) = [4 2; 1 3]
min max L = 3  
x    y
max min L = 2
y    x

Claim: max min L(x,y) <= min max L(x,y)
        y   x             x   y

        sup inf L(x,y) <= inf sup L(x,y)

sup = Least upper bound
inf = Greatest lower bound

Ex.
min 1/2 (w_1^2+w_2^2)
s.t. y_i (w_1 x_i1 + w_2 x_i2 + 1) >= 1

L(w,\alpha) = 1/2 (w_1^2+w_2^2 )
            + \alpha_1 (-w_1-w_2) + \alpha2 (-w_1-w_2+2)

w_1 = \alpha_1 + \alpha_2
w_2 = \alpha_1 + \alpha_2

L(\alpha) = - \alpha_1^2 - \alpha_2^2 - 2\alpha_1 \alpha_2 + 2 \alpha_2

-2\alpha_1 - 2\alpha_2 = 0
-2\alpha_2 - 2\alpha_1 + 2 = 0

D := {(x_i,y_i)}_i\in I
D := {([1,1],1), ([-1,-1],-1)} 

w_1+w_2>=0 | < w | [1 1] > >= 0
w_1+w_2>=2 | < w | [1 1] > >= 2

max     min     L(w, \alpha) 
alpha   w 
>=0

General Constrained Optimization Problem

minimize f(w)
s.t.    g_i(w)<=0, i=1,...,k
        h_i(w)=0, i=1,...,l

L(w, \alpha, \beta) 
    = f(w) + sum_i \alpha_i g_i(w) + sum_i \beta_i h_i(w)

---

Lecture 10 : 17/02/2025

Ex:
min 1/2 (w_1^2+w_2^2)
s.t. y_i (w_1 x_i1 + w_2 x_i2) >= 1

D := {(x_i,y_i)}_i\in I
D := {([1,1],1), ([-1,-1],-1)} 

Linear Programming : Simplex Method

Ex. 
minimize -x_1-x_2
s.t. x_1 + 2x_2 <= 3
2x_1 + x_2 <= 3
x_1, x_2 >= 0

min <c|x>
s.t. Ax >= b (#m)
x >= 0 (#n)
x\in \mathbb{R}^n
P  - feasible set 
A \in \mathbb{R}^{m\times n}
Rows of matrix A are linearly independent
A_i -> ith column of A 
a'_i -> ith row of A 

Definition:
Let x be an element of a polyhedron P. A vector d\in\mathbb{R}^n is
said to be a feasible direction at x, if there exists a positive 
scalar \theta for which x+\theta d \in P.

Definition:
Let P be a polyhedron. A vector x\in P is a vertex of P if there
exists some c such that <c|x> < <c|y> for all y satisfying y\in P and 
y \neq x.

P \subset \mathbb{R}^n
<a_i|x> >= b_i ; i\in M_1
<a_i|x> <= b_i ; i\in M_2
<a_i|x> = b_i ; i\in M_3

Definition:
If a vector x satisfies <a_i|x> = b_i for some i in M_1, M_2 or M_3,
we say that the corresponding constraint is active or binding at x.

Definition:
The vector x is a basic solution if:
(i) all equality constraints are active;
(ii) out of the constraints that are active at x, there are n of
them that are linearly independent.

*If x is a basic solution that satisfies all of the constraints,
we say that it is a basic feasible solution.

Let x  be a basic feasible solution to the standard form problem,
let B(1),...,B(m) be the indices of the basic variables, and let
BB = [A_B(1) ... A_B(m)] be the corresponding basis matrix.
(in particular x_i = 0 for every nonbasic variables)

x_B = inv(BB) b (vector of basic variables).






























