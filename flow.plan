CS316 Optimisation Methods
Monday & Wednesday 9:15 - 10:45 AM

Evaluation Policy
Mid Examination : 30% (Pen paper)
End Examination : 45% (Pen paper)
Term Paper : 25%

Books
Bertsimas, Dimitris, and John Tsitsiklis. Introduction to Linear Optimization. Belmont, MA: Athena Scientific, 1997. ISBN: 9781886529199.
David G. Luenberger and Yinyu Ye. Linear and Nonlinear Programming. Springer, 2008, Third Edition.

---
Lecture 1 : 05/01/2025
Example Linear Program
Normal Form Linear Program

Production Problem : Computer Manufacturer
Multiperiod Planning of Electric Power Capacity
Scheduling Problem
Choosing Paths in a Communication Network
Pattern Classification

Piecewise Linear Objective Function
Problems involving Absolute Values
Data Fitting
Optimal Control of Linear Systems
Rocket Control

Graphical Representation and Solution
Visualizing Standard Form Problems

Exercises (Chapter 1 of textbook)

---

Lecture 2 : 07/01/2025

Linear Program in Standard Form
Definitions:
Polyhedron, Halfspace, Hyperplane
Function and its epigraph
Convexity - Continuity?
Abstraction to metric space - convexity (vector space)
Convex hull, Convex combination
Extreme Point

---

Lecture 3 : 13/01/2025

Functions
f : \mathbb{R} \to \mathbb{R}, y = f(x)
Ex f(x) = ax

Continuity at a point
f is continuous at x_0 if \forall \epsilon>0 \exists \delta>0
such that, 
|f(x) - f(x_0)|< \epsilon \implies |x - x_0| < \delta.
-Extension-
d(f(x), f(x_0))<\epsilon \implies d(x, x_0)< \delta


Equality is defined in terms of inequality
x = y 
1 + 1/2 + 1/4 + 1/8 + ... = 2
|x-y|<\epsilon , \forall \epsilon>0

f:\mathbb{R}\to\mathbb{R}
f(x) = x^2
Minimum is in domain
Minimum value is in range
Local minimum vs global minimum

f(x) = (x-1)^2 
f(x) = (x-1)^2 + 5
f(x) = (x-1)^2 + (x+1)^2
f(x) = 2*x^2
f(x) = -x^2



f(x) = ax^2 a>0/ a<0
Derivative df/dx = 4x = 0 ??
Double derivative d^2f/dx^2 = -2

f(x) = x^Tx, x\in \mathbb{R^2}
    = [x_1 x_2][x_1 x_2]^T
    = x_1^2 + x_2^2
f(x) = x^TAx, x\in \mathbb{R^2}, A\in \mathbb{R^{2x2}}
A = [1 0; 0 3];
A = [-1 0; 0 -3];
A = [1 0; 0 -3];
---
Matlab (for plotting):
clear all;
close all;
x = [-10:.5:10];
A = [1 0; 0 -1];
for i = 1:length(x)
    for j = 1:length(x)
        f(i,j) = [x(i) x(j)]*A*[x(i) x(j)]';
    end
end
---

Lecture 4 : 15/01/2025

What if A has only one non-zero eigenvalue?
A = [1 0; 0 0]
f(x_1,x_2) = x_1^2
Another way to look at functions of two variables: 
Contour plot/ Level Set

f:\mathhb{R^n} \to \mathbb{R}
Lf(v) = {x | f(x)=v, x\in \mathbb{R^2}}

Important Concepts : Optimization
(A) Convexity
- Newton's Method (First & Second Derivatives)
- Gradient Method (First Derivative only)
 
(B) Lagrange Multipliers
- Theory of Duality

(C) Linear Program, Quadratic Program, SDP, Cone Programming

Argmin/ Min
argmin f(x) = value(s) of x where f reaches minimum

One variable: 

f(x), f'(x)=df/dx, ..., f^n(x) = (d/dx)^nf
f(x+dx) = f(x) + dx f'(x) + (1/2) (dx)^2 f''(x) + ... 
f(x+dx) ~ f(x) + dx f'(x) + (1/2) (dx)^2 f''(x) 

Two variables:
f : \mathbb{R^n} \to \mathbb{R}
f(x) := f(x_1,x_2)
f'(x) = Grad f(x) = [f_1(x) f_2(x)]' := Gf
f''(x) = [f_11(x) f_21(x); f_12(x) f_22(x)]' (Hessian of f) := Hf
f_21(x) = d/dx_2 (f_1(x)), where f_1(x) = d/dx_1 (f(x))

f(x+dx) ~ f(x) + dx f'(x) + (1/2) (dx)^2 f''(x) 
f(x+dx) ~ f(x) + dx^T Gf(x) + (1/2) dx^T Hf(x) dx

Ax = \lamda x
d/dx (e^ax) = a*(e^ax)

sin(2*pi*f*t) = (1/2i) * [e^(i*2*pi*f*t) - e^(-i*2*pi*f*t)]
d/dt (sin(2*pi*f*t)) = (pi*f) [e^(i*2*pi*f*t) + e^(-i*2*pi*f*t)]
 = (2*pi*f*e^(-i*pi/2)) sin(2*pi*f*t)

* Code to rectify!
clear all;
close all;

x = [-10:.5:10];
A = [1 0; 0 1];


for i = 1:length(x)
    for j = 1:length(x)
        f(i,j) = [x(i) x(j)]*A*[x(i) x(j)]';
        df1(i,j) = 2*x(i);    
        df2(i,j) = 2*x(j);
     end
end

[x1,x2] = meshgrid(x,x);
figure(1);
contour(x1,x2,f); hold on;
quiver(x1, x2, df1, df2);



Jacobian: J(x)
f : \mathbb{R^n} \to \mathbb{R^m}
m functions (f_1, f_2,..., f_m)
n variables (x_1, x_2, ..., x_n)

f(x+dx) ~ f(x) + J_f(x) dx
J_f(x) = [f_11 f_12 ... f_1n; f_21, ..., f_2n; ...; f_m1, ..., f_mn ]

!! Hessian of a real function of n variables is Jacobian of the Gradient
of the function.




















