CS316 Optimisation Methods
Monday & Wednesday 9:15 - 10:45 AM

Evaluation Policy
Mid Examination : 30% (Pen paper)
End Examination : 45% (Pen paper)
Term Paper : 25%

Books
Bertsimas, Dimitris, and John Tsitsiklis. Introduction to Linear Optimization. Belmont, MA: Athena Scientific, 1997. ISBN: 9781886529199.
David G. Luenberger and Yinyu Ye. Linear and Nonlinear Programming. Springer, 2008, Third Edition.

---
Lecture 1 : 05/01/2025
Example Linear Program
Normal Form Linear Program

Production Problem : Computer Manufacturer
Multiperiod Planning of Electric Power Capacity
Scheduling Problem
Choosing Paths in a Communication Network
Pattern Classification

Piecewise Linear Objective Function
Problems involving Absolute Values
Data Fitting
Optimal Control of Linear Systems
Rocket Control

Graphical Representation and Solution
Visualizing Standard Form Problems

Exercises (Chapter 1 of textbook)

---

Lecture 2 : 07/01/2025

Linear Program in Standard Form
Definitions:
Polyhedron, Halfspace, Hyperplane
Function and its epigraph
Convexity - Continuity?
Abstraction to metric space - convexity (vector space)
Convex hull, Convex combination
Extreme Point

---

Lecture 3 : 13/01/2025

Functions
f : \mathbb{R} \to \mathbb{R}, y = f(x)
Ex f(x) = ax

Continuity at a point
f is continuous at x_0 if \forall \epsilon>0 \exists \delta>0
such that, 
|f(x) - f(x_0)|< \epsilon \implies |x - x_0| < \delta.
-Extension-
d(f(x), f(x_0))<\epsilon \implies d(x, x_0)< \delta


Equality is defined in terms of inequality
x = y 
1 + 1/2 + 1/4 + 1/8 + ... = 2
|x-y|<\epsilon , \forall \epsilon>0

f:\mathbb{R}\to\mathbb{R}
f(x) = x^2
Minimum is in domain
Minimum value is in range
Local minimum vs global minimum

f(x) = (x-1)^2 
f(x) = (x-1)^2 + 5
f(x) = (x-1)^2 + (x+1)^2
f(x) = 2*x^2
f(x) = -x^2



f(x) = ax^2 a>0/ a<0
Derivative df/dx = 4x = 0 ??
Double derivative d^2f/dx^2 = -2

f(x) = x^Tx, x\in \mathbb{R^2}
    = [x_1 x_2][x_1 x_2]^T
    = x_1^2 + x_2^2
f(x) = x^TAx, x\in \mathbb{R^2}, A\in \mathbb{R^{2x2}}
A = [1 0; 0 3];
A = [-1 0; 0 -3];
A = [1 0; 0 -3];
---
Matlab (for plotting):
clear all;
close all;
x = [-10:.5:10];
A = [1 0; 0 -1];
for i = 1:length(x)
    for j = 1:length(x)
        f(i,j) = [x(i) x(j)]*A*[x(i) x(j)]';
    end
end
---

Lecture 4 : 15/01/2025

What if A has only one non-zero eigenvalue?
A = [1 0; 0 0]
f(x_1,x_2) = x_1^2
Another way to look at functions of two variables: 
Contour plot/ Level Set

f:\mathhb{R^n} \to \mathbb{R}
Lf(v) = {x | f(x)=v, x\in \mathbb{R^2}}

Important Concepts : Optimization
(A) Convexity
- Newton's Method (First & Second Derivatives)
- Gradient Method (First Derivative only)
 
(B) Lagrange Multipliers
- Theory of Duality

(C) Linear Program, Quadratic Program, SDP, Cone Programming

Argmin/ Min
argmin f(x) = value(s) of x where f reaches minimum

One variable: 

f(x), f'(x)=df/dx, ..., f^n(x) = (d/dx)^nf
f(x+dx) = f(x) + dx f'(x) + (1/2) (dx)^2 f''(x) + ... 
f(x+dx) ~ f(x) + dx f'(x) + (1/2) (dx)^2 f''(x) 

Two variables:
f : \mathbb{R^n} \to \mathbb{R}
f(x) := f(x_1,x_2)
f'(x) = Grad f(x) = [f_1(x) f_2(x)]' := Gf
f''(x) = [f_11(x) f_21(x); f_12(x) f_22(x)]' (Hessian of f) := Hf
f_21(x) = d/dx_2 (f_1(x)), where f_1(x) = d/dx_1 (f(x))

f(x+dx) ~ f(x) + dx f'(x) + (1/2) (dx)^2 f''(x) 
f(x+dx) ~ f(x) + dx^T Gf(x) + (1/2) dx^T Hf(x) dx

Ax = \lamda x
d/dx (e^ax) = a*(e^ax)

sin(2*pi*f*t) = (1/2i) * [e^(i*2*pi*f*t) - e^(-i*2*pi*f*t)]
d/dt (sin(2*pi*f*t)) = (pi*f) [e^(i*2*pi*f*t) + e^(-i*2*pi*f*t)]
 = (2*pi*f*e^(-i*pi/2)) sin(2*pi*f*t)

* Code to rectify!
clear all;
close all;

x = [-10:.5:10];
A = [1 0; 0 1];


for i = 1:length(x)
    for j = 1:length(x)
        f(i,j) = [x(i) x(j)]*A*[x(i) x(j)]';
        df1(i,j) = 2*x(i);    
        df2(i,j) = 2*x(j);
     end
end

[x1,x2] = meshgrid(x,x);
figure(1);
contour(x1,x2,f); hold on;
quiver(x1, x2, df1, df2);



Jacobian: J(x)
f : \mathbb{R^n} \to \mathbb{R^m}
m functions (f_1, f_2,..., f_m)
n variables (x_1, x_2, ..., x_n)

f(x+dx) ~ f(x) + J_f(x) dx
J_f(x) = [f_11 f_12 ... f_1n; f_21, ..., f_2n; ...; f_m1, ..., f_mn ]

!! Hessian of a real function of n variables is Jacobian of the Gradient
of the function.

---

Lecture 5 : 20/01/2025

Four fundamental subspaces of a Matrix
A : \mathbb{R^m} \to \mathbb{R^n}
A.shape = n x m

Span of a set of vectors
Rank of a Matrix - r
- Number of independent columns
- Number of independent rows
- Number of nonzero eigenvalues
- Dimension of column space

Column space of A := {y | y = Ax, x\in \mathbb{R^n}}, Span{Col(A)}
Left Null Space of A := {y | A^T y = 0, y\in \mathbb{R^m}}
Null space of A := {x | Ax = 0, x \in \mathbb{R^n}}
Row space of A := Span{Rows(A)}

n = dim(Null(A)) + r 
m = dim(Null(A^T)) + r

Ax_1 = y_1
Ax_0 = 0
A(x_1+x_0) = Ax_1 + Ax_0 = y_1

Gaussian Elimination
Trace of a Matrix
Singular Value Decomposition

---
Quadratic Unconstrained Optimization
f : \mathbb{R^2} \to \mathbb{R}
f(x) = x^T A x + b^T x + C
A \in \mathbb{R^{2x2}}, b\in \mathbb{R^2}, c\in \mathbb{R}

GD : First Attempt
f, df, x_0
x_{k+1} = x_k - step_size * df(x_k)
|| x_{k+1} - x_k || < espilon

Issues with stepsize and epsilon
- convergence?
- Gradient normalization?
- Cauchy sequences and sequence convergence

Neural Network
Training Set := {x_i, y_i}, x_i\in \mathbb{R^m}, y_i\in \mathbb{R^n}
f(x_i, \theta ) = y_i, \forall i \in \indx{Training Set} 

minimize \Sum_i ||f(x_i, \theta) - y_i||
f(x_i, a, b) = a*x_i + b
||f(x_i, \theta) - y_i||
\Sum_i (a*x_i + b - y_i)^2 
\phi(a,b) = x_i^2 * a^2 + b^2 + y_i^2 + 2*a*b*x_i - 2*a*x_i*y_i - 2*b*y_i

Q: Given three points in \mathbb{R^2}, {[1,  1], [2, 3], [4, 4]} 
1. Draw a straight line passing through these three points. 
2. Think of the points as [x_i, y_i] pairs. And straight line as a function 
relationship betwen x_i and y_i, such that f(x_i) = y_i
3. Write down the quadratic cost function.
4. Try solving the same using First Attempt GD.

---

Lecture 6 : 22/01/2025

Vector Space
- Linear Combinations
- Linear Independence of a set of vectors
- Functions from vector space to vector space
- Dimensionality of vector space
- Set of basis vector := basis
- Finite Dimensional Vector Space
- Representation theorem: Every linear function from FDV to FDV
- Linear functions as matrices
- Subspaces related to Matrix (Rol, Col, Null, Left Null)

Inner Product Space
- Orthogonality
- <x|y>, x,y \in V (Inner product)
- |x><y|, outer Product

Normed Vector Space
Shape of Vector Norms!
\mathcal{C}(\mathbb{R}) : {Set of all continuous real functions
on \mathbb{R}}

\mathcal((0,1))

- L_p (Lebesgue Integration)
- l_p  

V:=\mathbb{R^n}
l_p(v) = (sum_i (|v_i|)^p)^{1/p}

- circle -> sphere
- disk -> ball
- open disk -> open ball

* (0,1) [0,1) [0,1]

Ex: n=2, p=2
?{v | l_p(v) = 1} : = {v_1^2+v_2^2 = 1} (circle)
?{v | l_p(v) <= 1} is convex

Ex: n=2, p=1
?{v | l_p(v) <= 1} is convex
l_1(v) = sum_i abs(v_i)

Ex: n=2, p=3
?{v | l_p(v) <= 1} is convex

Ex: n=2, p=\infinity
l_\infinity(v) = sup(abs(v_i))
A = {0.1, .5, .7} 
A = {0, 1-1/2, 1-1/3, ..., 1-1/n, ... }
sup := least upper bound

Ex: n=2, p=0
?l_0(v) = 2
l_0(v) = |support(v)|
Sparsity norm * (misnormer)

Ex:
u = [1 2 3] 
v = [1 1 1]
p = {0, 1, 2, \inf }
||u-v||_p = {2, 3, sqrt(5), 2}

Ex:
S = {x | x_1+x_2 = 3}
Find a point in S closest to [0 0] 
w.r.t. p norms p:={0,1,2,\inf}

Limit points and boundary

---

Lecture 7 : 27/01/2025

A point is said to be inside the set (interior points) if
it has a neighborhood in the set.

Strict convexity (< inplace of <=)

Projection : P 
In the context of vector spaces - we have looked at 
projections on subspaces, i.e. hyperplanes passing through
origin

Ex. In \mathbb{R^2}, a^Tx = 0 a,x \in \mathbb{R^2}
a = [1 1]' - 1 D
a = [1 1 1]' - 2 D 
a = [1 1 1 1]' - 3 D 

Ex. Find a closest (l_2) point to origin on a^Tx+b=0, for a = [1 1 1]' 
in \mathbb{R^3} where b = 1

min ||x||^2
(A) s.t. a^Tx+b <= 0
(B) s.t. a^Tx+b >= 0
(C) s.t. a^Tx+b = 0

min x_1^2+x_2^2+x_3^2
s.t. x_1+x_2+x_3 + 1 = 0

<a|x> = a^Tx 

x=c*a such that this will be in a^Tx+b=0
a^T(c*a) + b = 0
c = -b/(a^Ta)
x = (-b/(a^Ta)) * a
x = (-b/||a||) * a_unit

Q Find a closest (l_2) point (wrt origin) on a^Tx+b=0, for a = [1 1 1]' 
in \mathbb{R^3} where b = 1
min ||x-[0 0 0]'||^2
s.t. a^Tx+b = 0
Ans: x = (-b/||a||) * a_unit

Q Find a closest (l_2) point (wrt x_0=[2 3 4]') on a^Tx+b=0, for a = [1 1 1]' 
in \mathbb{R^3} where b = 1
min ||x-x_0'||^2
s.t. a^Tx+b = 0
Ans is called projection of x_0 onto S:={x | a^Tx+b=0}
y = x-x_0'
min ||y||^2
s.t. a^T(y+x_0') + b =0
s.t. a^Ty + (b+a^Tx_0') = 0
s.t. a^Ty + b_ = 0
y = (-b_/a^Ta) * a
y = (-b-a^Tx_0 / a^Ta) * a
y = (-b/(a^Ta)) * a - (a_unit^Tx_0) * a_unit 

Quiz : Code
S_1 := {x | a^Tx + b = 0}
S_2 := {x | c^Tx + d = 0}
a, c \in \mathbb{R^2}, b, d \in \mathbb{R}

Start from x_0 and and sequentially project on S_1 and S_2 
till convergence
1. x_k := Projection of x_k-1 on S_1
2. x_k+1 := Projection of x_1=k on S_2 

A point, a convex set, a strict norm, and we get 
a unique projection!

---
Lecture 8 : 29/01/2025

{x_k}, k\in{0,1,...}
Rate of convergence: 
||x_k-x_{k-1}||/||x_{k-1}-x_{k-2}||

<x|y> = conjugate(x)^T * y, if x,y\in \mathbb{C}^n 
Visual Complex Analysis, Tristan Needham

*Neural Network - Learning Weights

min f(x)
s.t. <a|x>=b : S_1

f(x) = x^TAx, x\in\mathbb{R}^2
A = [1 1, 1 3];
a = [-1 3]^T;
b = 4;

Projected Gradient Descent
Initialize - x_0, lamda = 0.1
Step 1: x_1 = Project_S_1(x_0)
Step 2: x_2 = x_1 - 0.1*grad(f(x_1))
Repeat 

Matrix Derivatives
f(x), f'(x) 
f(x) = ax^2, f'(x) = 2*ax
f(x) = x^TAx, f'(x) = 2*A^Tx, if A is symmetric

f(x) = [x1 x2] * A * [x1 x2]^T 
= [x1a11+x2a21 x1a12+x2a22] * [x1 x2]^T 
= x1a11x1+x2a21x1+x1a12x2+x2a22x2

df/dx1 = 2a11x1 + (a21 + a12)x2 
df/dx2 = a21x1 + a12x1 + 2a22x2

df/dx = [2a11 a12+a21; a21+a12 2a22] * [x1 x2]^T
if A is symmetric a12 = a21
df/dx = 2[a11 a21; a12 a22]*x

(Matrix Cookbook!)

Single Neuron
x -> f_{w,b}() -> sigma(<x|w>+b)
{x_i, y_i} i\in I Training set
{{[1 1],  1}, {[2 3], 1}, {[-1 0], -1}}
sigma(x) = x 

min ||y_1 - f_{w,b}(x_1)|| + ...
s.t. ||w||=1

\phi(x,y) = ||y-f(x)||
min \phi(x_1,y_1) + \phi(x_2,y_2) + ...

min \phi_1(w,b) + \phi_2(w,b) + \phi_3(w,b) + ...
s.t ||w||=1

Initialization : x_0, 
Till convergence
for i=1:3
Step 1: x_1 = x_0 - lamda * grad(\phi_i)
Step 2: x_2 = Project_S_1(x_1)
Repeat

---

Lecture 9 : 10/02/2025

Ex. 
min 1/2 x^2
s.t 2x >= 1

(Dual Variable - \alpha)

L(x,\alpha) = 1/2 x^2 + \alpha (-2x+1), \alpha>=0
? max L(x,\alpha)
s.t. \alpha >= 0
x \in feasible, what is \alpha? \alpha = 0
max L(x,0) = 1/2 x^2 -> objective function!
x \in infeasible, what is \alpha? \alpha = \infinity
max L(x,\infinity) = \infinity

min     max     L(x, \alpha)
x       \alpha>=0 

max     min     L(x, \alpha)
\alpha>=0 x


L(x,y) = [4 2; 1 3]
min max L = 3  
x    y
max min L = 2
y    x

Claim: max min L(x,y) <= min max L(x,y)
        y   x             x   y

        sup inf L(x,y) <= inf sup L(x,y)

sup = Least upper bound
inf = Greatest lower bound

Ex.
min 1/2 (w_1^2+w_2^2)
s.t. y_i (w_1 x_i1 + w_2 x_i2 + 1) >= 1

L(w,\alpha) = 1/2 (w_1^2+w_2^2 )
            + \alpha_1 (-w_1-w_2) + \alpha2 (-w_1-w_2+2)

w_1 = \alpha_1 + \alpha_2
w_2 = \alpha_1 + \alpha_2

L(\alpha) = - \alpha_1^2 - \alpha_2^2 - 2\alpha_1 \alpha_2 + 2 \alpha_2

-2\alpha_1 - 2\alpha_2 = 0
-2\alpha_2 - 2\alpha_1 + 2 = 0

D := {(x_i,y_i)}_i\in I
D := {([1,1],1), ([-1,-1],-1)} 

w_1+w_2>=0 | < w | [1 1] > >= 0
w_1+w_2>=2 | < w | [1 1] > >= 2

max     min     L(w, \alpha) 
alpha   w 
>=0

General Constrained Optimization Problem

minimize f(w)
s.t.    g_i(w)<=0, i=1,...,k
        h_i(w)=0, i=1,...,l

L(w, \alpha, \beta) 
    = f(w) + sum_i \alpha_i g_i(w) + sum_i \beta_i h_i(w)

---

Lecture 10 : 17/02/2025

Ex:
min 1/2 (w_1^2+w_2^2)
s.t. y_i (w_1 x_i1 + w_2 x_i2) >= 1

D := {(x_i,y_i)}_i\in I
D := {([1,1],1), ([-1,-1],-1)} 

Linear Programming : Simplex Method

Ex. 
minimize -x_1-x_2
s.t. x_1 + 2x_2 <= 3
2x_1 + x_2 <= 3
x_1, x_2 >= 0

min <c|x>
s.t. Ax >= b (#m)
x >= 0 (#n)
x\in \mathbb{R}^n
P  - feasible set 
A \in \mathbb{R}^{m\times n}
Rows of matrix A are linearly independent
A_i -> ith column of A 
a'_i -> ith row of A 

Standard Form LP
min <c|x> 
s.t. Ax = b 
x >= 0

Definition:
Let x be an element of a polyhedron P. A vector d\in\mathbb{R}^n is
said to be a feasible direction at x, if there exists a positive 
scalar \theta for which x+\theta d \in P.

Definition:
Let P be a polyhedron. A vector x\in P is a vertex of P if there
exists some c such that <c|x> < <c|y> for all y satisfying y\in P and 
y \neq x.

P \subset \mathbb{R}^n
<a_i|x> >= b_i ; i\in M_1
<a_i|x> <= b_i ; i\in M_2
<a_i|x> = b_i ; i\in M_3

Definition:
If a vector x satisfies <a_i|x> = b_i for some i in M_1, M_2 or M_3,
we say that the corresponding constraint is active or binding at x.

Definition:
The vector x is a basic solution if:
(i) all equality constraints are active;
(ii) out of the constraints that are active at x, there are n of
them that are linearly independent.

*If x is a basic solution that satisfies all of the constraints,
we say that it is a basic feasible solution.

Let x  be a basic feasible solution to the standard form problem,
let B(1),...,B(m) be the indices of the basic variables, and let
BB = [A_B(1) ... A_B(m)] be the corresponding basis matrix.
(in particular x_i = 0 for every nonbasic variables)

Ax = b 
A|basic x|basic = b 
x_B = inv(BB) b (vector of basic variables).

---
Lecture 11 : 19/02/2025

Projections of polyhedra:

[1] P = {x | Ax >= b, x\in\mathbb{R}^n}
[2] P = {x' | Ax' = b, x'>=0, x'\in \mathbb{R}^n}
(Standard form of polyhedron)

Ex:
P : = {2x_1 + x_2 >= 3, x_1 + 2x_2 >= 7, x1 + x2 <=10}
\pi_2{P} ?
Graphically!
Algebraically? (Eliminate x_1)
x_1 >= -x_2/2 + 3/2 
x_1 >= -2x_2 + 7
10 - x_2 >= x_1
==
10 - x_2 >= -x_2/2 + 3/2
10 - x_2 >= -2x_2 + 7
==
x_2 <= 17
x_2 >= -3
==
-3 <= x_2 <= 17
Question: \pi_1{P} = {-7 <= x_1 <= 13}

Equivalent (additional variables!)
2x1 + x2 - x3 = 3
x1 + 2x1 - x4 = 7

Definition: Projection \pi_k : \mathbb{R}^n \to mathbb{R}^k
will project x onto its first k coordinates:
\pi_k(x) = \pi_k(x_1,...,x_n) = (x_1,...,x_k)

\pi_k(S) = {\pi_k(x) | x\in S}

Note: S is nonempty iff \pi_k(S) is nonempty.

Ex: 
P={x_1 + x_2 >= 1,
x_1 + x_2 + 2x_3 >= 2,
2x_1 + 3x_3 >= 3,
x_1 - 4x_3 >= 4,
-2x_1 + x_2 - x_3 >= 5}
Q: Is the polyhedron nonempty?
\pi_3(P)
                    0   >= 1 - x_1 - x_2
                    x_3 >= 1 - x_1/2 - x_2/2
                    x_3 >= 1 - 2x_1/3
x_1/4 - 1       >=  x_3
-2x_1 + x_2 - 5 >=  x_3
===
            0   >= 1 - x_1 - x_2
x_1/4 - 1       >= 1 - x_1/2 - x_2/2
x_1/4 - 1       >= 1 - 2x_1/3
-2x_1 + x_2 - 5 >= 1 - x_1/2 - x_2/2
-2x_1 + x_2 - 5 >= 1 - 2x_1/3 
===
x_1 >= 1 - x_2
x_1 >= 8/3 - 2x_2/3 
x_1 >= 24/11
x_2 - 4 >= x_1
3x_2/4 - 9/2 >= x_1
===
x_2 - 4 >= 1 - x_2
x_2 - 4 >= 8/3 - 2x_2/3 
x_2 - 4 >= 24/11
3x_2/4 - 9/2 >= 1 - x_2
3x_2/4 - 9/2 >= 8/3 - 2x_2/3 
3x_2/4 - 9/2 >= 24/11
===
x_2 >= 98/11

* Fourier-Motzkin Elimination

Observations:
[1] Let P\subset \mathbb{R}^{n+k} be a polyhedron, then
{x\in \mathbb{R}^n | \exists y\in \mathbb{R}^k, s.t. (x,y)\in P}
is also a polyhedron.

[2] Let P\subset \mathbb{R}^n be a polyhedron and 
let A\in\mathbb{R}^{m\times n} then
Q = { Ax | x\in P} is a polyhedron.

Suppose the polyhedron P = {x\in \mathbb{R}^n | Ax>=b, A_{mxn}} is
nonempty,
The following are equivalent:
1. The polyhedron P does not contain a line
2. The polyhedron P has atleast one extreme point
3. There exists n vectors out of the family a_1,...,a_m which are 
linearly independent.

---

Lecture 12: March 3, 2025

Mid-sem Review
Q1 y_1, y_2 \in S_1
< a y_1 + (1-a) y_2 | x > 
= <a y_1 | x> + <(1-a)y_2 | x>
= a <y_1 | x> + (1-a) <y_2|x>
>= a 100 + (1-a) 100
>= 100

Project y onto x, x,y \in \mathbb{R}^n
Px(y) = < y | x/||x|| > x/||x||
if y is not in S1*
    1. y' = y - Px(y)
    2. y'' = y' + k x/||x||
        s.t. y'' \in S1* boundary
    <y'' | x> = 100
    <y' + k x/||x|| | x> = 100
    k = (100 - <y'|x>)/||x||

x = [1 0 0 0 0 1 0 1 0 0 0 1] x^ = 1/2 x
y = [100 100 100 100 25 0 0 0 10 20 30 40]
1. y' = y - 35 x = [65 100 100 100 25 -35 0 -35 10 20 30 5]
2. k = (100 - <y-35x|x>)/2
    = (100 - <y|x> + 35<x|x>)/2
    = (100 - 140 + 140 )/2 = 50
y'' = (y-35x)+50x/2 = y - 35x + 25x = y - 10x
y'' = [90 100 100 100 25 -10 0 -10 10 20 30 30]

Ex : 
minimize -10 x_1 - 12 x_2 - 12 x_3
subject to 
    x_1 + 2x_2 + 2x_3 <= 20 
    2x_1 + x_2 + 2x_3 <= 20
    2x_1 + 2x_2 + x_3 <= 20
    x_1, x_2, x_3 >= 0

minimize -10 x_1 - 12 x_2 - 12 x_3 
subject to 
    x_1 + 2x_2 + 2x_3 + x_4                 = 20 
    2x_1 + x_2 + 2x_3        + x_5          = 20
    2x_1 + 2x_2 + x_3               +x_6    = 20
    x_1, x_2, x_3, x_4, x_5, x_6 >= 0

B(1) = 4, B(2) = 5, B(3) = 6
BB x_B = [20 20 20]^T 
A basic solution
x = [0 0 0 20 20 20]^T
Another basic solution
x = [4 4 4 0 0 0]^T

---

Lecture 13: March 4, 2025

Simplex (Naive Attempt)
minimize -4 x_1 - 3 x_2
subject to 
        2 x_1 + 3 x_2 <= 6     (1)
        -3 x_1 + 2 x_2 <= 3     (2)
                2 x_2 <= 5      (3)
        2 x_1 + x_2 <= 4        (4)
        x_1, x_2 >= 0           (5, 6)

Step 1 From given set of six equations (equality) choose an
arbitrary pair of equations. Solve these equations to obtain the 
coordinates of their intersection.

Step 2 If the solution is feasible, then it is a corner-point 
solution. Otherwise, discard it.

Step 3 Go to step 1 unless all combinations are studied.

Infeasibility: In general, the feasible region could be empty.
Procedure to solve LP is meaningful only if feasible region is nonempty.
If it is empty, LP is s.t.b. infeasible.

Unboundedness: x_1>=0, x_2>=0 -> region which is unbounded

LP in Standard Form 
minimize -4 x_1 - 3 x_2
subject to 
    2 x_1 + 3 x_2 + x_3                     = 6
    -3 x_1 + 2 x_2      + x_4               = 3
            2 x_2               +x_5        = 5
    2 x_1 + x_2                        +x_6 = 4
    x_1, x_2, x_3, x_4, x_5, x_6 >= 0

From Graph to Standard Form
(x_1,x_2)->(x_1,...,x_6)
(1,1) -> (1,1,1,4,3,1)

From standard form to Original LP
(x_1,...,x_6)->(x_1,x_2)
(0,0,6,5,3,4)->(0,0)

m constraints, n variables
select n-m nonbasic variables = 0
m basic variables 
Total n equations n unknowns!

Revised Step 1
Choose an arbitrary pair of variables in the augmented (SFLP)
and assign value zero to those variables. This will reduce
functional constraints in the augmented problem to a set of 
four equations in four unknowns. Solve this system of equations.

Revised Step 2
If all the values in the augmented solution produced by the 
revised step 1 are nonnegative, accept it as a corner-point solution;
otherwise discard it.

Ex. x_3 = x_6 = 0 (nonbasic variables)
2 x_1 + 3 x_2                       = 6
-3 x_1 + 2 x_2      + x4            = 3
        2 x_2 +         + x_5       = 5
2 x_1 + x_2                         = 4
Basic feasible solution: (3/2, 1, 0, 11/2, 3, 0)

Anology : Basic - Nonbasic (Playing - Bench)

Ex. x_1 and x_2 as nonbasic variables
Basic feasible solution : x = (0,0,6,3,5,4)
Ax = b
x+D = (0,d,6-3d,3-2d,5-2d,4-d)
x+D = x +  d (0,1,0,0,0,0) -  d (0,0,A_2')
A(x+D) = b 
Ax + AD = b 
b + AD = b
AD = 0 (D is in the null space of A)
A(0,d,-3d,-2d,-2d,-d)=0
d=min(6/3, 3/2, 5/2, 4) and d>=0
d = 3/2
x_ = (0,3/2, 3/2, 0, 2, 5/2)

-----
March 17, 2025
Lecture 14:
x = (0,3/2, 3/2, 0, 2, 5/2)
x+D = (d, 3(1+d)/2, (3-13d)/2, 0, (2-3d), (5-7d)/2)
d = 3/13
x+D = x_ = (3/13, 24/13, 0, 0, 2-9/13, 5/2-21/26)

Question?
Basic feasible solution : Point A : x = (0,0,6,3,5,4)
along x_2 point B -> (0,3/2,3/2,0,2,5/2) -> -9/2
along x_1 point E -> (2,0,2,9,5,0) -> -8
Quiz: What if we travel along x_2 starting from E?

Simplex Example:
minimize -10 x_1 - 12 x_2 - 12 x_3
subject to 
        x_1 + 2 x_2 + 2 x_3 <= 20
        2 x_1 + x_2 + 2 x_3 <= 20
        2 x_1 + 2 x_2 + x_3 <= 20
        x_1, x_2, x_3 >= 0
subject to
        x_1 + 2 x_2 + 2 x_3 + x_4 = 20
        2 x_1 + x_2 + 2 x_3 + x_5 = 20
        2 x_1 + 2 x_2 + x_3 + x_6 = 20
        x_1, x_2, x_3, x_4, x_5, x_6 >= 0
(0,0,0,20,20,20)
        x_1     x_2     x_3     x_4     x_5     x_6
    0   -10     -12     -12     0       0       0    
----------------------------------------------------
x_4 20  1       2       2       1       0       0
x_5 20  2       1       2       0       1       0
x_6 20  2       2       1       0       0       1    

An iteration:
1. We start with a basis consisting of the basic columns
A_B(1), ..., A_B(m), an associated basic feasible solution
x, and the inverse B^{-1} of the basis matrix.

2. Compute the row vector p'= c_B' B^{-1} and then Compute
the reduced costs c_j_ = c_j - p' A_j. If they are all 
nonnegative, the current basic feasible solution is optimal,
and the algorithm terminates; else, choose some j for which
c_j_ < 0.

3. Compute u = B^{-1} A_j. If no component of u is positive,
the optimal cost is -\infinity, and the algorithm terminates.

4. If some component of u is positive, let
\theta* = min x_B(i)/u_i, {i=1,..,m | u_i>0}

5. Let l be such that \theat* = x_B(l)/u_l. Form a new basis
by replacing A_B(l) with A_j. If y is the new basic feasible
solution, the values of the new basic variables are y_j = \theta*
and y_B(i) = x_B(i)-\theta* u_i, i\neq l.

-----
Lecture 15:

Duality (Just for illustration, one of many interpretations)

Ex: Transportation Graph:= (V,E)
({A, B, C, D}, {AB-1, AC-2, BC-3, BD-4, CD-5})
Some good is available at A with no cost.
Cost of moving a unit of good along edges c_i
At each vertex b_v units of goods are sold. 
Question: How can the transport be done most efficiently?

Let x_i denote the amount of good tranported through channel i.
(Primal Problem - P)
minimize c'x 
subject to
    x_1 - x_3 - x_4 = b_B
    x_2 + x_3 - x_5 = b_C
    x_4 + x_5 = b_D
    x_1 + x_2 = b_B + b_C + b_D # need not include explicitly
    x_i >= 0

Let y_v denote the unit price of the good at node v
(Dual Problem - D)
maximize b'y
subject to
    y_B - y_A <= c_1
    y_C - y_A <= c_2
    y_C - y_B <= c_3
    y_D - y_B <= c_4
    y_D - y_C <= c_5

Claim:
Assume that x is a feasible vector for the primal problem (P)
and y is a feasible vector for the dual problem (D).
i. b'y <= c'x
ii. if equality holds in (i), then x and y are optimal for 
their respective linear programming problems

(P) minimize c'x subject to Ax  >= b, x>=0
(D) maximize b'y subject to A'y <= c, y>=0

Ax >= b
A'y <= c 
y'Ax >= y'b --- (A)
(y'A) <= c'
y'Ax <= c'x --- (B)
y'b <= c'x



 


















